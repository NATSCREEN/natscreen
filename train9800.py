import sys
import os
from datetime import datetime
from pure_data_loader import prepare_pure_training_dataset  # ä½¿ç”¨100%çº¯Pythonæ•°æ®åŠ è½½å™¨
from Transformer import MolecularTransformer
from operations_T import sigmoid
from autograd_T import no_grad
from optimizer_T import Adam
from new_focal_loss import FocalLoss
from tensor_T import Tensor  # ç¡®ä¿ä½¿ç”¨è‡ªå®šä¹‰Tensor

class TeeOutput:
    """åŒæ—¶è¾“å‡ºåˆ°ç»ˆç«¯å’Œæ–‡ä»¶çš„ç±»"""
    def __init__(self, filename):
        self.terminal = sys.stdout
        self.log_file = open(filename, 'w', encoding='utf-8')
        
    def write(self, message):
        self.terminal.write(message)
        self.log_file.write(message)
        self.log_file.flush()  # ç«‹å³å†™å…¥æ–‡ä»¶
        
    def flush(self):
        self.terminal.flush()
        self.log_file.flush()
        
    def close(self):
        self.log_file.close()

def setup_logging():
    """è®¾ç½®æ—¥å¿—è¾“å‡ºåˆ°æ–‡ä»¶"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"training_log_latest.txt"
    
    # é‡å®šå‘stdoutåˆ°åŒæ—¶è¾“å‡ºåˆ°ç»ˆç«¯å’Œæ–‡ä»¶
    tee = TeeOutput(log_filename)
    sys.stdout = tee
    
    print(f"ðŸŽ¯ è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åˆ°: {log_filename}")
    print(f"ðŸ“… å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    return tee, log_filename

def initialize_network_and_optimizer(optimal_parameters, activation='gelu'):
    """Initialize network and optimizer with optimal parameters"""
    print(f"ðŸŽ¯ åˆ›å»ºç½‘ç»œ - æ¿€æ´»å‡½æ•°: {activation.upper()}")
    network = MolecularTransformer(
        input_features=2048,#2048
        output_features=1, 
        embedding_size=128,#512
        layer_count=optimal_parameters['transformer_depth'],
        head_count=optimal_parameters['attention_heads'], 
        hidden_size=optimal_parameters['hidden_dimension'], 
        dropout_rate=0.1,
        activation=activation  # æ–°å¢žæ¿€æ´»å‡½æ•°å‚æ•°
    )
    optimizer = Adam(network.parameters(), lr=optimal_parameters['learning_rate'])
    return network, optimizer

def conduct_individual_training(network, data_handler, optimizer, network_index, model_version, unique_id):
    """Train individual network"""
    print(f"=== å¼€å§‹è®­ç»ƒç½‘ç»œ {unique_id+1} ===")
    
    criterion = FocalLoss(alpha=0.25, gamma=2.0, high_pred_penalty=5.0, reduction='mean')  # é™ä½Žæƒ©ç½šç³»æ•°
    
    for epoch in range(1):  # è®­ç»ƒ2ä¸ªepoch
        epoch_losses = []
        batch_count = 0
        high_pred_false_count = 0
        very_high_pred_false_count = 0 
        extreme_high_pred_false_count = 0
        all_predictions = []
        
        # è®­ç»ƒæ¯ä¸ªæ‰¹æ¬¡
        for batch_idx, (features, labels) in enumerate(data_handler):
            batch_count += 1
            
            # ç¡®ä¿è¾“å…¥æ•°æ®æ˜¯æˆ‘ä»¬çš„è‡ªå®šä¹‰Tensor
            if not isinstance(features, Tensor):
                features = Tensor(features, requires_grad=False)
            if not isinstance(labels, Tensor):
                labels = Tensor(labels, requires_grad=False)
            
            # å‰å‘ä¼ æ’­
            outputs = network(features)  
            
            # ç¡®ä¿æ ‡ç­¾ç»´åº¦æ­£ç¡®
            if labels.data.ndim > 1:
                labels = Tensor(labels.data.squeeze(), requires_grad=False)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(outputs.squeeze(), labels)
            
            # å¼‚å¸¸æŸå¤±æ£€æµ‹
            loss_value = loss.data.item() if hasattr(loss.data, 'item') else float(loss.data)
            if loss_value > 5.0: 
                print(f"âš ï¸ å¼‚å¸¸é«˜æŸå¤± {loss_value:.4f}ï¼Œè·³è¿‡å‚æ•°æ›´æ–°")
                continue
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            epoch_losses.append(loss_value)
            
            # è®¡ç®—é¢„æµ‹å‡†ç¡®æ€§ç»Ÿè®¡
            with no_grad():
                predictions = sigmoid(outputs.squeeze())
                all_predictions.extend(predictions.data.flatten().tolist())
                
                pred_data = predictions.data.flatten()
                label_data = labels.data.flatten()
                
                for pred, label in zip(pred_data, label_data):
                    if pred > 0.9 and label < 0.5:
                        high_pred_false_count += 1
                    if pred > 0.95 and label < 0.5:
                        very_high_pred_false_count += 1
                    if pred > 0.98 and label < 0.5:
                        extreme_high_pred_false_count += 1
            
            # æ¯10ä¸ªæ‰¹æ¬¡æ‰“å°æŸå¤±
            if batch_count % 10 == 0:
                print(f"    æ‰¹æ¬¡ {batch_count}, æŸå¤±: {loss_value:.4f}")
        
        # Epochæ€»ç»“
        avg_loss = sum(epoch_losses) / len(epoch_losses) if epoch_losses else 0
        print(f"  Epoch [{epoch+1}], å¹³å‡æŸå¤±: {avg_loss:.4f}, "
              f"High Pred False: {high_pred_false_count}, "
              f"Very High Pred False: {very_high_pred_false_count}, "
              f"Extreme High Pred False: {extreme_high_pred_false_count}")
        
        # æ‰“å°é¢„æµ‹å€¼èŒƒå›´
        if all_predictions:
            min_pred = min(all_predictions)
            max_pred = max(all_predictions)
            print(f"  [DEBUG] Epoch {epoch+1} é¢„æµ‹å€¼èŒƒå›´: [{min_pred:.4f}, {max_pred:.4f}]")
    
    # ä¿å­˜æ¨¡åž‹
    model_filename = f'model_{unique_id+1}.dict'
    USE_PICKLE_FORMAT = True  # ä½¿ç”¨pickleæ ¼å¼ä¿å­˜
    print(f"[SAVE] ä¿å­˜æ ¼å¼: {'pickle(å¿«é€Ÿ)' if USE_PICKLE_FORMAT else 'model_serializer(çº¯Python)'}")
    
    if USE_PICKLE_FORMAT:
        print(f"[SAVE] ä½¿ç”¨pickleæ ¼å¼ä¿å­˜æ¨¡åž‹åˆ°: {model_filename}")
        import pickle
        save_data = {
            'model_parameters': network.state_dict(),
            'optimizer_state': optimizer.state_dict(),
            'epoch': 2
        }
        with open(model_filename, 'wb') as f:
            pickle.dump(save_data, f)
    else:
        print(f"[SAVE] ä½¿ç”¨model_serializeræ ¼å¼ä¿å­˜æ¨¡åž‹åˆ°: {model_filename}")
        from utils2 import store_model
        store_model(network, optimizer, 2, model_filename, use_pickle=False)
    
    print(f"âœ… ç½‘ç»œ {unique_id+1} è®­ç»ƒå®Œæˆï¼Œæ¨¡åž‹å·²ä¿å­˜åˆ° {model_filename}")
    return network

def training():
    """Main function for network training - Sequential version"""
    # è®¾ç½®æ—¥å¿—è¾“å‡º
    tee, log_filename = setup_logging()
    
    # è®¾ç½®å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿ç»“æžœå¯é‡çŽ°
    import pure_random
    FIXED_SEED = 42  # ä½¿ç”¨å›ºå®šç§å­
    pure_random.seed(FIXED_SEED)
    print(f"ðŸŽ² ä½¿ç”¨å›ºå®šéšæœºç§å­: {FIXED_SEED} (ç¡®ä¿ç»“æžœå¯é‡çŽ°)")
    print("=" * 60)
    
    try:
        from command_line_parser import CommandLineProcessor  
        
        config_parser = CommandLineProcessor(description='Molecular property prediction')
        config_parser.add_argument('--num_networks', type=int, default=1, 
                                 help='Quantity of networks to train')
        config_parser.add_argument('--activation', type=str, default='gelu', choices=['relu', 'gelu'],
                                 help='é€‰æ‹©æ¿€æ´»å‡½æ•°: relu æˆ– gelu (é»˜è®¤: gelu)')
        parameters = config_parser.parse_args()

        print(f'ðŸš€ å¼€å§‹é¡ºåºè®­ç»ƒ {parameters.num_networks} ä¸ªç½‘ç»œ')
        print(f'âš™ï¸  æ¿€æ´»å‡½æ•°è®¾ç½®: {parameters.activation.upper()}')
        
        if parameters.activation.lower() == 'gelu':
            print("âœ¨ GELUæ¿€æ´»å‡½æ•°é€šå¸¸åœ¨Transformeræž¶æž„ä¸­è¡¨çŽ°æ›´ä½³")
        else:
            print("âš¡ ReLUæ¿€æ´»å‡½æ•°è®¡ç®—é€Ÿåº¦æ›´å¿«ï¼Œèµ„æºæ¶ˆè€—æ›´å°‘")

        # ä½¿ç”¨100%çº¯Pythonæ•°æ®åŠ è½½å™¨
        print("æ­£åœ¨å‡†å¤‡æ•°æ®é›†...")
        #batch_size=10 #32
        data_handler = prepare_pure_training_dataset('training_dataset.csv', fingerprint_type='Morgan', 
                                                   batch_size=32, shuffle=False, balance_data=True)
        print("âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ")
        
        # ä¼˜åŒ–å‚æ•°é…ç½®
        optimal_parameters = {
            'learning_rate': 0.001,
            'transformer_depth': 2,   #æ”¹æˆ4ï¼ˆåŽŸæ¥6ï¼‰
            'attention_heads': 2, 
            'hidden_dimension': 64   #æ”¹æˆ512ï¼ˆåŽŸæ¥2048ï¼‰
        }

        # åˆå§‹åŒ–å’Œè®­ç»ƒç½‘ç»œ
        print("æ­£åœ¨åˆå§‹åŒ–ç½‘ç»œ...")
        trained_networks = []
        
        for network_idx in range(parameters.num_networks):
            print(f'\n--- åˆå§‹åŒ–ç½‘ç»œ {network_idx+1} ---')
            network, optimizer = initialize_network_and_optimizer(optimal_parameters, parameters.activation)
            
            # è®­ç»ƒæ­¤ç½‘ç»œ
            trained_network = conduct_individual_training(
                network, data_handler, optimizer, 0, 2, network_idx
            )
            trained_networks.append(trained_network)

        print(f"\nðŸŽ‰ æ‰€æœ‰è®­ç»ƒå®Œæˆï¼æˆåŠŸè®­ç»ƒäº† {len(trained_networks)} ä¸ªç½‘ç»œ")
        print(f"æ¨¡åž‹æ–‡ä»¶: {[f'model_{i+1}.dict' for i in range(len(trained_networks))]}")
        print(f"ðŸ”§ ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°: {parameters.activation.upper()}")
        
        print("=" * 60)
        print(f"ðŸ“„ è®­ç»ƒæ—¥å¿—å·²ä¿å­˜åˆ°: {log_filename}")
        print(f"ðŸ• ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        # æ¢å¤åŽŸå§‹stdoutå¹¶å…³é—­æ—¥å¿—æ–‡ä»¶
        sys.stdout = tee.terminal
        tee.close()
        print(f"âœ… æ—¥å¿—å·²ä¿å­˜åˆ°æ–‡ä»¶: {log_filename}")

if __name__ == "__main__":
    training()
